{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ur8xi4C7S06n"
      },
      "outputs": [],
      "source": [
        "# Copyright 2023 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JAPoU8Sm5E6e"
      },
      "source": [
        "# Vertex AI Experiments: Autologging"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "24743cf4a1e1"
      },
      "source": [
        "**_NOTE_**: This notebook has been tested in the following environment:\n",
        "\n",
        "* Python version = 3.10.2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tvgnzT1CKxrO",
        "tags": []
      },
      "source": [
        "## Overview\n",
        "\n",
        "As part of the data science team, you want to try different modeling approaches during experimentation phase.To guarantee reproducibility, each approach has different parameters that you need to manually track This is a time consuming task. To address this challenge, Vertex AI SDK introduces autologging, a one-line code SDK capability which leverages MLflow to provide automatic metrics and parameters tracking associated with your  Vertex AI Experiments and experiment runs. Learn more about [Autologging data to an experiment run](https://cloud.google.com/vertex-ai/docs/experiments/autolog-data)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d975e698c9a4"
      },
      "source": [
        "### Objective\n",
        "\n",
        "In this tutorial, you learn how to use `Vertex AI Autologging`.\n",
        "\n",
        "This tutorial uses the following Google Cloud ML services and resources:\n",
        "\n",
        "- Vertex AI Experiments\n",
        "\n",
        "The steps performed include:\n",
        "\n",
        "- Enable autologging in the Vertex AI SDK.\n",
        "- Train one scikit-learn model\n",
        "- Train one LightGBM model\n",
        "- Train three XGBoost models (setting different hyperparameters)\n",
        "- See all the resulting experiment runs with the evaluation metrics (accuracy, precision, recall, and F1-score) and parameters autologged to Vertex AI Experiments without setting any experiment run.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "08d289fa873f"
      },
      "source": [
        "### Dataset\n",
        "\n",
        "The dataset is the [UCI Car Evaluation dataset](https://archive-beta.ics.uci.edu/dataset/19/car+evaluation), which is derived from simple hierarchical decision model and it contains attributions to predict car evaluation class."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aed92deeb4a0"
      },
      "source": [
        "### Costs\n",
        "\n",
        "This tutorial uses billable components of Google Cloud:\n",
        "\n",
        "* Vertex AI Experiments\n",
        "* Vertex AI Tensorboard\n",
        "* Cloud Storage\n",
        "\n",
        "Learn about [Vertex AI pricing](https://cloud.google.com/vertex-ai/pricing),\n",
        "and [Cloud Storage pricing](https://cloud.google.com/storage/pricing),\n",
        "and use the [Pricing Calculator](https://cloud.google.com/products/calculator/)\n",
        "to generate a cost estimate based on your projected usage."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i7EUnXsZhAGF"
      },
      "source": [
        "## Installation\n",
        "\n",
        "Install the following packages required to execute this notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2b4ef9b72d43"
      },
      "outputs": [],
      "source": [
        "# Install the packages\n",
        "USER = \"\"\n",
        "! pip3 install {USER} --upgrade google-cloud-aiplatform tensorflow\n",
        "! pip3 install {USER} --upgrade pandas scikit-learn category_encoders torch torchdata torchmetrics mlflow==2.4.0 lightgbm\n",
        "! pip3 install {USER} --upgrade protobuf==3.20.3\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "58707a750154"
      },
      "source": [
        "### Colab only: Uncomment the following cell to restart the kernel.\n",
        "Automatically restart kernel after installs so that your environment can access the new packages"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import IPython\n",
        "\n",
        "# app = IPython.Application.instance()\n",
        "# app.kernel.do_shutdown(True)"
      ],
      "metadata": {
        "id": "O6T0JnTqnpEA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BF1j6f9HApxa"
      },
      "source": [
        "## Setup\n",
        "\n",
        "### Define env variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oM1iC_MfAts1",
        "outputId": "372e78ec-fa0f-4e82-f199-b8bb0ce30a74"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "12106.47s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "E0619 14:32:15.362065081  127664 backup_poller.cc:136]                 Run client channel backup poller: UNKNOWN:pollset_work {created_time:\"2023-06-19T14:32:15.361979318+00:00\", children:[UNKNOWN:Bad file descriptor {created_time:\"2023-06-19T14:32:15.361929472+00:00\", errno:9, os_error:\"Bad file descriptor\", syscall:\"epoll_wait\"}]}\n",
            "Updated property [core/project].\n"
          ]
        }
      ],
      "source": [
        "# Set the project id\n",
        "ROJECT_ID = \"my_project\"  # @param {type:\"string\"}\n",
        "! gcloud config set project {PROJECT_ID}\n",
        "\n",
        "# Set your region\n",
        "REGION = \"europe-west4\"  # @param {type: \"string\"}\n",
        "\n",
        "# Set a bucket to store intermediate artifacts\n",
        "BUCKET_URI = \"gs://my_bucket\"  # @param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-EcIXiGsCePi"
      },
      "source": [
        "**Only if your bucket doesn't already exist**: Run the following cell to create your Cloud Storage bucket."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NIq7R4HZCfIc"
      },
      "outputs": [],
      "source": [
        "# ! gsutil mb -l $REGION -p $PROJECT_ID $BUCKET_URI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Suk_ZwEyPR1X"
      },
      "source": [
        "### Upload dataset to BigQuery, load it to a pandas dataframe\n",
        "\n",
        "1. Upload the [ UCI Car Evaluation dataset](https://archive-beta.ics.uci.edu/dataset/19/car+evaluation) (mentioned above) to a BigQuery table (**your turn**)\n",
        "2. Upload data from this table to a pandas dataframe.\n",
        "\n",
        "(you could also interact with the data in CSV format if you prefer, but then you'd need to do some minor changes to the code)\n",
        "\n",
        "#### 2. Upload BigQuery table to a pandas dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HNeNo37hQSIW"
      },
      "outputs": [],
      "source": [
        "%load_ext google.cloud.bigquery\n",
        "\n",
        "from google.cloud import bigquery\n",
        "client = bigquery.Client()\n",
        "\n",
        "## JUST FOR COLAB\n",
        "#from google.colab import data_table\n",
        "#data_table.enable_dataframe_formatter()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TbcgndESPQAf"
      },
      "outputs": [],
      "source": [
        "%%bigquery cars_df --project $PROJECT_ID\n",
        "\n",
        "SELECT * FROM `my_project.my_dataset.my_table` # @param {type: \"string\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vs8hk1r4mZVq"
      },
      "outputs": [],
      "source": [
        "cars_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "960505627ddf"
      },
      "source": [
        "### Import libraries\n",
        "\n",
        "Import the Vertex AI SDK to log experiments in Vertex AI Experiments."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PyQmSRbKA8r-"
      },
      "outputs": [],
      "source": [
        "from google.cloud import aiplatform as vertex_ai"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2mwQMym-Rteu"
      },
      "source": [
        "### Helper functions\n",
        "\n",
        "To run experiments it is not uncommon to define experiment helpers, one per each modelling approach you plan to evaluate. Below you define the following experiment helpers:\n",
        "\n",
        "*   `train_sklearn_model`: A helper function to train a Decision Tree model using Sklearn.\n",
        "*   `train_xgboost_model`: A helper function to train a Decision Tree model using XGBoost.\n",
        "*   `train_LGBM_model`: A helper function to train a Decision Tree model using LightGBM.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T5tWyGgeRvXC"
      },
      "outputs": [],
      "source": [
        "def set_seed(seed: int):\n",
        "    \"\"\"\n",
        "    A function to set the seed for reproducibility.\n",
        "    Args:\n",
        "        seed: Seed to be set\n",
        "    Returns:\n",
        "        None\n",
        "    \"\"\"\n",
        "    import random\n",
        "\n",
        "    import numpy as np\n",
        "    import tensorflow as tf\n",
        "\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    tf.random.set_seed(seed)\n",
        "\n",
        "\n",
        "\n",
        "def train_sklearn_model(df: pd.DataFrame, test_size: int, max_depth: int):\n",
        "    \"\"\"\n",
        "    A function to train a Decision Tree model using sklearn.\n",
        "    Args:\n",
        "        df: Input data, which needs to be split into train and test\n",
        "        test_size: Size of the test set\n",
        "        max_depth: Maximum depth of the Decision Tree\n",
        "    Returns:\n",
        "        None\n",
        "    \"\"\"\n",
        "\n",
        "    # Libraries\n",
        "    import pandas as pd\n",
        "    from category_encoders import OrdinalEncoder\n",
        "    from sklearn import metrics\n",
        "    from sklearn.model_selection import train_test_split\n",
        "    from sklearn.pipeline import Pipeline\n",
        "    from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "    # Train, test split\n",
        "    print(\"Generating train and test data...\")\n",
        "    x = df[[\"buying\", \"maint\", \"doors\", \"persons\", \"lug_boot\", \"safety\"]]\n",
        "    y = df[[\"class\"]]\n",
        "    x_train, x_test, y_train, y_test = train_test_split(\n",
        "        x, y, test_size=test_size, shuffle=True\n",
        "    )\n",
        "\n",
        "    # Build pipeline\n",
        "    print(\"Building pipeline...\")\n",
        "    pipe = Pipeline(\n",
        "        [\n",
        "            (\"encoder\", OrdinalEncoder()),\n",
        "            (\"model\", DecisionTreeClassifier(criterion=\"gini\", max_depth=max_depth)),\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    # Train model\n",
        "    print(\"Training model...\")\n",
        "    pipe.fit(x_train, y_train)\n",
        "\n",
        "    # Evaluate model\n",
        "    print(\"Evaluating model...\")\n",
        "    y_pred = pipe.predict(x_test)\n",
        "    accuracy = metrics.accuracy_score(y_test, y_pred)\n",
        "    print(\"Accuracy:\", round(accuracy, 3))\n",
        "    precision = metrics.precision_score(y_test, y_pred)\n",
        "    print(\"Precision:\", round(precision, 3))\n",
        "    recall = metrics.recall_score(y_test, y_pred)\n",
        "    print(\"Recall:\", round(recall, 3))\n",
        "    f1 = metrics.f1_score(y_test, y_pred)\n",
        "    print(\"F1 score:\", round(f1, 3))\n",
        "\n",
        "\n",
        "def train_xgboost_model(df: pd.DataFrame, test_size: int, max_depth: int, n_estimators: int,  enable_categorical: bool=True):\n",
        "    \"\"\"\n",
        "    A function to train a Decision Tree model using XGboost.\n",
        "    Args:\n",
        "        df: Input data, which needs to be split into train and test\n",
        "        test_size: Size of the test set\n",
        "        max_depth: Maximum depth of the Decision Tree\n",
        "        n_estimators: Number of trees\n",
        "        enable_categorical: Whether to enable categorical features\n",
        "    Returns:\n",
        "        None\n",
        "    \"\"\"\n",
        "\n",
        "    # Libraries\n",
        "    import pandas as pd\n",
        "    from xgboost import XGBClassifier\n",
        "    from sklearn import metrics\n",
        "    from sklearn.model_selection import train_test_split\n",
        "    from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "\n",
        "    # Convert categorical columns to numerical values\n",
        "    for column in cars_df.select_dtypes(include='object').columns:\n",
        "        label_encoder = LabelEncoder()\n",
        "        cars_df[column] = label_encoder.fit_transform(cars_df[column])\n",
        "\n",
        "    # Train, test split\n",
        "    print(\"Generating train and test data...\")\n",
        "    x = cars_df[[\"buying\", \"maint\", \"doors\", \"persons\", \"lug_boot\", \"safety\"]]\n",
        "    y = cars_df[[\"class\"]]\n",
        "    x_train, x_test, y_train, y_test = train_test_split(\n",
        "        x, y, test_size=test_size, shuffle=True\n",
        "    )\n",
        "\n",
        "    # Train model\n",
        "    print(\"Training model...\")\n",
        "    xgb_clf = XGBClassifier(\n",
        "        max_depth=max_depth, n_estimators=n_estimators, objective=\"binary:logistic\"\n",
        "    )\n",
        "    xgb_clf.fit(x_train, y_train)\n",
        "\n",
        "    # Evaluate model\n",
        "    print(\"Evaluating model...\")\n",
        "    y_pred = xgb_clf.predict(x_test)\n",
        "    accuracy = metrics.accuracy_score(y_test, y_pred)\n",
        "    print(\"Accuracy:\", round(accuracy, 3))\n",
        "    precision = metrics.precision_score(y_test, y_pred)\n",
        "    print(\"Precision:\", round(precision, 3))\n",
        "    recall = metrics.recall_score(y_test, y_pred)\n",
        "    print(\"Recall:\", round(recall, 3))\n",
        "    f1 = metrics.f1_score(y_test, y_pred)\n",
        "    print(\"F1 score:\", round(f1, 3))\n",
        "\n",
        "\n",
        "\n",
        "def train_lgbm_model(df, test_size=0.25, max_depth=3, n_estimators=100):\n",
        "    \"\"\"\n",
        "    A function to train a Light GBM model.\n",
        "    Args:\n",
        "        df: Input data, which needs to be split into train and test\n",
        "        test_size: Size of the test set\n",
        "        max_depth: Maximum depth of the Decision Tree\n",
        "        n_estimators: Number of trees\n",
        "        enable_categorical: Whether to enable categorical features\n",
        "    Returns:\n",
        "        None\n",
        "    \"\"\"\n",
        "\n",
        "    # Libraries\n",
        "    import pandas as pd\n",
        "    import lightgbm as lgb\n",
        "    from sklearn import metrics\n",
        "    from sklearn.metrics import accuracy_score\n",
        "    from sklearn.model_selection import train_test_split\n",
        "    from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "    # Convert categorical columns to numerical values\n",
        "    for column in cars_df.select_dtypes(include='object').columns:\n",
        "        label_encoder = LabelEncoder()\n",
        "        cars_df[column] = label_encoder.fit_transform(cars_df[column])\n",
        "\n",
        "    # Train, test split\n",
        "    print(\"Generating train and test data...\")\n",
        "    x = cars_df[[\"buying\", \"maint\", \"doors\", \"persons\", \"lug_boot\", \"safety\"]]\n",
        "    y = cars_df[[\"class\"]]\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        x, y, test_size=test_size, shuffle=True\n",
        "    )\n",
        "\n",
        "    # Create a label encoder for the target variable\n",
        "    label_encoder = LabelEncoder()\n",
        "    y_train = label_encoder.fit_transform(y_train)\n",
        "    y_test = label_encoder.transform(y_test)\n",
        "\n",
        "    # Create a LightGBM model\n",
        "    model = lgb.LGBMClassifier(objective='binary', max_depth=max_depth, n_estimators=n_estimators)\n",
        "\n",
        "    # Train the model on the train set\n",
        "    print(\"Training model...\")\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # Evaluate the model on the test set\n",
        "    y_pred = model.predict(X_test)\n",
        "    accuracy = metrics.accuracy_score(y_test, y_pred)\n",
        "    print(\"Accuracy:\", round(accuracy, 3))\n",
        "    precision = metrics.precision_score(y_test, y_pred)\n",
        "    print(\"Precision:\", round(precision, 3))\n",
        "    recall = metrics.recall_score(y_test, y_pred)\n",
        "    print(\"Recall:\", round(recall, 3))\n",
        "    f1 = metrics.f1_score(y_test, y_pred)\n",
        "    print(\"F1 score:\", round(f1, 3))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "init_aip:mbsdk,all"
      },
      "source": [
        "### Initialize Vertex AI SDK for Python and set seed for reproducibility\n",
        "\n",
        "Initialize the Vertex AI SDK for Python for your project and set seed to guarantee reproducibility."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G1l-7Wft3jb6"
      },
      "outputs": [],
      "source": [
        "vertex_ai.init(project=PROJECT_ID, location=REGION, staging_bucket=BUCKET_URI)\n",
        "set_seed(8)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "004aGQ1BSLFm"
      },
      "source": [
        "## Model experimentation using autologging with Vertex AI Experiments\n",
        "\n",
        "Vertex AI Experiments Autologging allows you to run experiments and autologging parameters and metrics of different ML frameworks.\n",
        "\n",
        "After initiating an Vertex AI Experiment, enable autologging using `vertex_ai.autolog()`.\n",
        "\n",
        "There are two ways to use Autologging:\n",
        "\n",
        "1.   *With automatic experiment run creation*\n",
        "2.   *With user experiment run creation*\n",
        "\n",
        "With *automatic experiment run creation*, you run an experiment. Vertex AI SDK automatically creates an experiment run by logging all paramenters and metrics in Vertex AI Experiments.\n",
        "\n",
        "With *user experiment run creation*, you create an experiment using `vertex_ai.start_run(your-experiment-run-name)` and run the experiment. Then you get access to resulting paramentes and metrics after you end the experiment run with `vertex_ai.end_run()`\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JjJVARe5V-MG"
      },
      "source": [
        "#### Create an experiment for tracking training parameters and metrics\n",
        "\n",
        "To start, initiate an experiment using the `init()` method.\n",
        "\n",
        "Because some model types like TensorFlow result in autologging time series metrics, you need to create a TensorBoard instance.\n",
        "\n",
        "To create a TensorBoard instance, you can use `vertex_ai.Tensorboard.create()`.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mRoiPPk1XCWi"
      },
      "source": [
        " ⚠⚠⚠ **Warning** Currently, Vertex AI TensorBoard charges a monthly fee of 300 USD per unique active user. Beginning in August 2023, Vertex AI TensorBoard will change from a per-user monthly license of 300 USD per month to 10 USD per GiB per month for storage of logs and metrics. This means there are no more subscription fees, and you will only pay for the storage you've used. See the [Vertex AI TensorBoard: Delete Outdated TensorBoard Experiments tutorial](https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/official/experiments/delete_outdated_tensorboard_experiments.ipynb) for how to manage storage. More info on Tensorboard [here](https://cloud.google.com/vertex-ai/docs/experiments/tensorboard-overview).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r2z-VaIVTtI5"
      },
      "outputs": [],
      "source": [
        "AUTOLOGGED_EXPERIMENT_NAME = \"my-experiment\"  # @param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Trjm4Dq3yH_o"
      },
      "outputs": [],
      "source": [
        "EXPERIMENT_TENSORBOARD = vertex_ai.Tensorboard.create()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eqM2pIO8_4Y7"
      },
      "outputs": [],
      "source": [
        "vertex_ai.init(\n",
        "    project=PROJECT_ID,\n",
        "    location=REGION,\n",
        "    staging_bucket=BUCKET_URI,\n",
        "    experiment=AUTOLOGGED_EXPERIMENT_NAME,\n",
        "    experiment_tensorboard=EXPERIMENT_TENSORBOARD,\n",
        "    experiment_description=\"autolog-experiment-with-automatic-run\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LCoYnaY594V7"
      },
      "source": [
        "#### Autologging an experiment with automatic experiment run creation\n",
        "\n",
        "In this section, Vertex AI SDK automatically creates an experiment run for you by logging all paramenters and training and post-training metrics in Vertex AI Experiments.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8VGUnqk6Zvb6"
      },
      "source": [
        "##### Enable autologging\n",
        "\n",
        "First, enable autologging using `vertex_ai.autolog()` method.\n",
        "\n",
        "After calling `vertex_ai.autolog()`, any metrics and parameters from\n",
        "model training calls with supported ML frameworks will be automatically\n",
        "logged to Vertex Experiments."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k0H9kw74-5ob"
      },
      "outputs": [],
      "source": [
        "vertex_ai.autolog()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yWWr08gZZzMb"
      },
      "source": [
        "#### Experiment 1 - Sklearn model\n",
        "\n",
        "Next, define your baseline model by running a Sklearn model experiment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aLCVSnA_zbpN"
      },
      "outputs": [],
      "source": [
        "train_sklearn_model(cars_df, test_size=0.2, max_depth=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cQh3v_85aGWP"
      },
      "source": [
        "##### Get the experiment results\n",
        "\n",
        "Then, use the method `get_experiment_df()` to get the results of the experiment as a pandas dataframe.\n",
        "\n",
        "Notice how all paramenters and metrics are logged in Vertex AI Experiments.\n",
        "\n",
        "In particular, the `run_name` has been automatically assigned and the evaluation metrics you defined have been logged too."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KuAl1etkLwAe"
      },
      "outputs": [],
      "source": [
        "experiment_df = vertex_ai.get_experiment_df()\n",
        "experiment_df = experiment_df.T\n",
        "experiment_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qQlwVV_VMjGZ"
      },
      "source": [
        "#### Experiment 2 - LGBM model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qOu7DxpVVMMY"
      },
      "outputs": [],
      "source": [
        "train_lgbm_model(cars_df, test_size=0.2, max_depth=3, n_estimators=50)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mFcmm9y107Ag"
      },
      "source": [
        "#### Experiment 3 - XGBoost model\n",
        "\n",
        "Let's train 3 models, changing the parameters max_depth and n_estimators."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NSladWsI04jh"
      },
      "outputs": [],
      "source": [
        "train_xgboost_model(cars_df, test_size=0.2, max_depth=3, n_estimators=50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zEfU4BHzyAM0"
      },
      "outputs": [],
      "source": [
        "train_xgboost_model(cars_df, test_size=0.2, max_depth=5, n_estimators=100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TgMhMXfTyAM0"
      },
      "outputs": [],
      "source": [
        "train_xgboost_model(cars_df, test_size=0.2, max_depth=10, n_estimators=200)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hAQUlys2j0UN"
      },
      "source": [
        "#### Compare experiment results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2XcCQUCgdTmG"
      },
      "outputs": [],
      "source": [
        "experiment_df = vertex_ai.get_experiment_df()\n",
        "experiment_df.T"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "environment": {
      "kernel": "conda-root-py",
      "name": "workbench-notebooks.m106",
      "type": "gcloud",
      "uri": "gcr.io/deeplearning-platform-release/workbench-notebooks:m106"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "conda-root-py"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}